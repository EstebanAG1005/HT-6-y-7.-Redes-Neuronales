{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab0a15e3-9dbf-4cf5-ae1f-77337a63fec6",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-15T20:21:51.468117+00:00",
          "start_time": "2023-05-15T20:21:49.130239+00:00"
        },
        "noteable": {
          "cell_type": "code"
        },
        "noteable-chatgpt": {
          "version": "0.16.0"
        }
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "observaciones = 1000\n",
        "\n",
        "x1 = np.random.uniform(low=-10, high=10, size=(observaciones,1))\n",
        "x2 = np.random.uniform(-10, 10, (observaciones,1))\n",
        "\n",
        "X = np.column_stack((x1,x2))\n",
        "\n",
        "ruido = np.random.uniform(-1, 1, (observaciones,1))\n",
        "\n",
        "metas = 2 * x1 - 3 * x2 + 5 + ruido\n",
        "\n",
        "x1N = x1.reshape(observaciones,)\n",
        "x2N = x2.reshape(observaciones,)\n",
        "metasN = metas.reshape(observaciones,)\n",
        "\n",
        "fig = px.scatter_3d(x = x1N, y = x2N, z = metasN)\n",
        "\n",
        "fig.update_layout(\n",
        "    width = 500,\n",
        "    height = 500,)\n",
        "\n",
        "fig.show()\n",
        "\n",
        "rango_inicial = 0.1\n",
        "\n",
        "pesos = np.random.uniform(low = -rango_inicial, high = rango_inicial, size=(2, 1))\n",
        "\n",
        "sesgos = np.random.uniform(low = -rango_inicial, high = rango_inicial, size=1)\n",
        "\n",
        "pesos.shape\n",
        "\n",
        "eta = 0.01\n",
        "\n",
        "for i in range (100):\n",
        "    y = np.dot(X, pesos) + sesgos\n",
        "    deltas = y - metas\n",
        "    perdida = np.sum(deltas ** 2) / 2 / observaciones\n",
        "    print(perdida)\n",
        "    deltas_escaladas = deltas / observaciones\n",
        "    pesos = pesos - eta * np.dot(X.T, deltas_escaladas)\n",
        "    sesgos = sesgos - eta * np.sum(deltas_escaladas)\n",
        "\n",
        "print(pesos, sesgos)\n",
        "\n",
        "yN = y.reshape(observaciones,)\n",
        "metasN = metas.reshape(observaciones,)\n",
        "fig = px.scatter(x = yN, y =  metasN)\n",
        "\n",
        "fig.update_layout(\n",
        "    width = 400,\n",
        "    height = 400,)\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01c87be0-01bd-4df8-9dcd-a4146b219f15",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-15T20:26:44.471200+00:00",
          "start_time": "2023-05-15T20:26:39.253156+00:00"
        },
        "noteable": {
          "cell_type": "code"
        },
        "noteable-chatgpt": {
          "version": "0.16.0"
        }
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# Cambiando las observaciones a 100,000\n",
        "\n",
        "observaciones = 100000\n",
        "\n",
        "x1 = np.random.uniform(low=-10, high=10, size=(observaciones,1))\n",
        "x2 = np.random.uniform(-10, 10, (observaciones,1))\n",
        "\n",
        "X = np.column_stack((x1,x2))\n",
        "\n",
        "ruido = np.random.uniform(-1, 1, (observaciones,1))\n",
        "\n",
        "metas = 2 * x1 - 3 * x2 + 5 + ruido\n",
        "\n",
        "x1N = x1.reshape(observaciones,)\n",
        "x2N = x2.reshape(observaciones,)\n",
        "metasN = metas.reshape(observaciones,)\n",
        "\n",
        "fig = px.scatter_3d(x = x1N, y = x2N, z = metasN)\n",
        "\n",
        "fig.update_layout(\n",
        "    width = 500,\n",
        "    height = 500,)\n",
        "\n",
        "fig.show()\n",
        "\n",
        "rango_inicial = 0.1\n",
        "\n",
        "pesos = np.random.uniform(low = -rango_inicial, high = rango_inicial, size=(2, 1))\n",
        "\n",
        "sesgos = np.random.uniform(low = -rango_inicial, high = rango_inicial, size=1)\n",
        "\n",
        "pesos.shape\n",
        "\n",
        "eta = 0.01\n",
        "\n",
        "for i in range (100):\n",
        "    y = np.dot(X, pesos) + sesgos\n",
        "    deltas = y - metas\n",
        "    perdida = np.sum(deltas ** 2) / 2 / observaciones\n",
        "    print(perdida)\n",
        "    deltas_escaladas = deltas / observaciones\n",
        "    pesos = pesos - eta * np.dot(X.T, deltas_escaladas)\n",
        "    sesgos = sesgos - eta * np.sum(deltas_escaladas)\n",
        "\n",
        "print(pesos, sesgos)\n",
        "\n",
        "yN = y.reshape(observaciones,)\n",
        "metasN = metas.reshape(observaciones,)\n",
        "fig = px.scatter(x = yN, y =  metasN)\n",
        "\n",
        "fig.update_layout(\n",
        "    width = 400,\n",
        "    height = 400,)\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "b207e939",
      "metadata": {},
      "source": [
        "### 1. Cambie el número de observaciones a 100,000. Explique que es lo que ocurre en términos de:\n",
        "\n",
        "#### 1. El tiempo de ejecución para resolver el problemas\n",
        "\n",
        "    El tiempo de ejecución aumenta de 0.4seg a 1.5seg lo cual indica que al ser mas los casos de observación la corrida aumenta significativamente.\n",
        "\n",
        "#### 2. El resultado final vrs lo encontrado en clase: es igual, o diferente...¿porqué?\n",
        "\n",
        "    \n",
        "#### 3. Las graficas para representar los datos/resultados\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17ca89e4-e7c8-48ef-8bde-463426067667",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-15T20:39:32.791405+00:00",
          "start_time": "2023-05-15T20:39:29.581877+00:00"
        },
        "noteable": {
          "cell_type": "code"
        },
        "noteable-chatgpt": {
          "version": "0.16.0"
        }
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# Cambiando las observaciones a 1,000,000\n",
        "\n",
        "observaciones = 1000000\n",
        "\n",
        "x1 = np.random.uniform(low=-10, high=10, size=(observaciones,1))\n",
        "x2 = np.random.uniform(-10, 10, (observaciones,1))\n",
        "\n",
        "X = np.column_stack((x1,x2))\n",
        "\n",
        "ruido = np.random.uniform(-1, 1, (observaciones,1))\n",
        "\n",
        "metas = 2 * x1 - 3 * x2 + 5 + ruido\n",
        "\n",
        "x1N = x1.reshape(observaciones,)\n",
        "x2N = x2.reshape(observaciones,)\n",
        "metasN = metas.reshape(observaciones,)\n",
        "\n",
        "fig = px.scatter_3d(x = x1N, y = x2N, z = metasN)\n",
        "\n",
        "fig.update_layout(\n",
        "    width = 500,\n",
        "    height = 500,)\n",
        "\n",
        "fig.show()\n",
        "\n",
        "rango_inicial = 0.1\n",
        "\n",
        "pesos = np.random.uniform(low = -rango_inicial, high = rango_inicial, size=(2, 1))\n",
        "\n",
        "sesgos = np.random.uniform(low = -rango_inicial, high = rango_inicial, size=1)\n",
        "\n",
        "pesos.shape\n",
        "\n",
        "eta = 0.01\n",
        "\n",
        "for i in range (100):\n",
        "    y = np.dot(X, pesos) + sesgos\n",
        "    deltas = y - metas\n",
        "    perdida = np.sum(deltas ** 2) / 2 / observaciones\n",
        "    print(perdida)\n",
        "    deltas_escaladas = deltas / observaciones\n",
        "    pesos = pesos - eta * np.dot(X.T, deltas_escaladas)\n",
        "    sesgos = sesgos - eta * np.sum(deltas_escaladas)\n",
        "\n",
        "print(pesos, sesgos)\n",
        "\n",
        "yN = y.reshape(observaciones,)\n",
        "metasN = metas.reshape(observaciones,)\n",
        "fig = px.scatter(x = yN, y =  metasN)\n",
        "\n",
        "fig.update_layout(\n",
        "    width = 400,\n",
        "    height = 400,)\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "dbbeff62",
      "metadata": {},
      "source": [
        "### 2. Cambie el número de observaciones a 1,000,000. Explique que es lo que ocurre en términos de:\n",
        "\n",
        "#### 1. El tiempo de ejecución para resolver el problemas\n",
        "\n",
        "    El tiempo de ejecución aumenta de 0.4seg a 1.5seg lo cual indica que al ser mas los casos de observación la corrida aumenta significativamente.\n",
        "\n",
        "#### 2. El resultado final vrs lo encontrado en clase: es igual, o diferente...¿porqué?\n",
        "\n",
        "    \n",
        "#### 3. Las graficas para representar los datos/resultados\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "79b9fa8e",
      "metadata": {},
      "source": [
        "### 3. “Juegue” un poco con el valor de la tasa de aprendizaje, por ejemplo 0.0001, 0.001, 0.1, 1.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db79e052",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import time  # Importamos la biblioteca time\n",
        "\n",
        "# Cambiando las observaciones a 1,000,000\n",
        "observaciones = 1000000\n",
        "\n",
        "x1 = np.random.uniform(low=-10, high=10, size=(observaciones,1))\n",
        "x2 = np.random.uniform(-10, 10, (observaciones,1))\n",
        "\n",
        "X = np.column_stack((x1,x2))\n",
        "\n",
        "ruido = np.random.uniform(-1, 1, (observaciones,1))\n",
        "metas = 2 * x1 - 3 * x2 + 5 + ruido\n",
        "\n",
        "x1N = x1.reshape(observaciones,)\n",
        "x2N = x2.reshape(observaciones,)\n",
        "metasN = metas.reshape(observaciones,)\n",
        "\n",
        "fig = px.scatter_3d(x = x1N, y = x2N, z = metasN)\n",
        "fig.update_layout(width = 500, height = 500)\n",
        "fig.show()\n",
        "\n",
        "rango_inicial = 0.1\n",
        "\n",
        "etas = [0.0001, 0.001, 0.01, 0.1, 1]  # diferentes tasas de aprendizaje a probar\n",
        "\n",
        "for eta in etas:\n",
        "    start_time = time.time()  # Comenzamos a contar el tiempo\n",
        "\n",
        "    pesos = np.random.uniform(low = -rango_inicial, high = rango_inicial, size=(2, 1))\n",
        "    sesgos = np.random.uniform(low = -rango_inicial, high = rango_inicial, size=1)\n",
        "\n",
        "    for i in range (100):\n",
        "        y = np.dot(X, pesos) + sesgos\n",
        "        deltas = y - metas\n",
        "        perdida = np.sum(deltas ** 2) / 2 / observaciones\n",
        "        deltas_escaladas = deltas / observaciones\n",
        "        pesos = pesos - eta * np.dot(X.T, deltas_escaladas)\n",
        "        sesgos = sesgos - eta * np.sum(deltas_escaladas)\n",
        "\n",
        "    end_time = time.time()  # Terminamos de contar el tiempo\n",
        "    print(f\"Tiempo de ejecución con tasa de aprendizaje {eta}: {end_time - start_time} segundos\")\n",
        "\n",
        "    print(f\"Resultados con tasa de aprendizaje {eta} - Pesos: {pesos}, Sesgos: {sesgos}\")\n",
        "\n",
        "    yN = y.reshape(observaciones,)\n",
        "    metasN = metas.reshape(observaciones,)\n",
        "    fig = px.scatter(x = yN, y =  metasN)\n",
        "    fig.update_layout(width = 400, height = 400)\n",
        "    fig.show()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "78d9d481",
      "metadata": {},
      "source": [
        "#### Para cada uno de estos indique:\n",
        "### 1. ¿Qué ocurre con el tiempo de ejecución?\n",
        "\n",
        "    En teoría cuando la tasa de aprendizaje es muy baja (por ejemplo, 0.0001), el tiempo de ejecución será más largo porque el algoritmo necesita más iteraciones para converger al mínimo. Por otro lado, si la tasa de aprendizaje es muy alta (por ejemplo, 1), el tiempo de ejecución puede ser más corto, pero el algoritmo puede no converger en absoluto o incluso divergir. Más sin embargo con los datos que obtuvimos estos tiempos fueron muy parecidos.\n",
        "\n",
        "### 2. ¿Qué ocurre con la minimización de la pérdida?\n",
        "\n",
        "    Si la tasa de aprendizaje es muy baja, la pérdida se minimiza muy lentamente. Si la tasa de aprendizaje es muy alta, la pérdida puede no minimizarse en absoluto y el algoritmo puede incluso divergir, lo que significa que la pérdida puede aumentar en lugar de disminuir.\n",
        "    \n",
        "### 3. ¿Qué ocurre con los pesos y los sesgos?\n",
        "\n",
        "    Los pesos y los sesgos se actualizan en cada iteración en función de la tasa de aprendizaje y los gradientes. Si la tasa de aprendizaje es muy baja, los pesos y los sesgos cambian muy lentamente. Si la tasa de aprendizaje es muy alta, los pesos y los sesgos pueden cambiar rápidamente, pero el algoritmo puede no converger a los valores óptimos.\n",
        "    \n",
        "### 4. ¿Qué ocurre con las iteraciones?\n",
        "\n",
        "    Con una tasa de aprendizaje baja, necesitarás más iteraciones para alcanzar la convergencia. Con una tasa de aprendizaje alta, puedes alcanzar la convergencia en menos iteraciones, pero también corres el riesgo de no converger en absoluto o de divergir.\n",
        "\n",
        "### 5. ¿El problema queda resuelto o no?\n",
        "\n",
        "    Si queda resuelto ya que vemos como entre menor es el eta el resultado es mejor, aunque es un poco mas tardado. mas sin embargo un valor de eta muy bajo vemos como la relación ya no parece una relación lineal ya que en la mitad se ve como una montaña la cual hace que el modelo no esté bien entrenado. Por lo que tener un eta promedio no tan bajo pero tampoco tan alto nos de una mejor respuesta a nuestro modelo.\n",
        "\n",
        "### 6. ¿Cuál es la apariencia de la última gráfica? ¿Se cumple con la condición de que sea de 45 grados?\n",
        "\n",
        "    Para la ultima grafica ni siquiera se logra ver la relación lineal, y en la penultima grafica se ve una relación lineal de 90 grados por lo cual para estos dos etas no fue entrenado de manera correcta. \n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "2db43f33",
      "metadata": {},
      "source": [
        "### 4 Cambie la función de pérdida “L2-norm” a la misma pero sin dividir por 2. Explique lo que ocurre en términos de:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47428cdb",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# Cambiando las observaciones a 1000\n",
        "\n",
        "observaciones = 1000\n",
        "\n",
        "x1 = np.random.uniform(low=-10, high=10, size=(observaciones,1))\n",
        "x2 = np.random.uniform(-10, 10, (observaciones,1))\n",
        "\n",
        "X = np.column_stack((x1,x2))\n",
        "\n",
        "ruido = np.random.uniform(-1, 1, (observaciones,1))\n",
        "\n",
        "metas = 2 * x1 - 3 * x2 + 5 + ruido\n",
        "\n",
        "x1N = x1.reshape(observaciones,)\n",
        "x2N = x2.reshape(observaciones,)\n",
        "metasN = metas.reshape(observaciones,)\n",
        "\n",
        "fig = px.scatter_3d(x = x1N, y = x2N, z = metasN)\n",
        "\n",
        "fig.update_layout(\n",
        "    width = 500,\n",
        "    height = 500,)\n",
        "\n",
        "fig.show()\n",
        "\n",
        "rango_inicial = 0.1\n",
        "\n",
        "pesos = np.random.uniform(low = -rango_inicial, high = rango_inicial, size=(2, 1))\n",
        "\n",
        "sesgos = np.random.uniform(low = -rango_inicial, high = rango_inicial, size=1)\n",
        "\n",
        "pesos.shape\n",
        "\n",
        "eta = 0.01\n",
        "\n",
        "for i in range (100):\n",
        "    y = np.dot(X, pesos) + sesgos\n",
        "    deltas = y - metas\n",
        "    perdida = np.sum(deltas ** 2) / observaciones\n",
        "    print(perdida)\n",
        "    deltas_escaladas = deltas / observaciones\n",
        "    pesos = pesos - eta * np.dot(X.T, deltas_escaladas)\n",
        "    sesgos = sesgos - eta * np.sum(deltas_escaladas)\n",
        "\n",
        "print(pesos, sesgos)\n",
        "\n",
        "yN = y.reshape(observaciones,)\n",
        "metasN = metas.reshape(observaciones,)\n",
        "fig = px.scatter(x = yN, y =  metasN)\n",
        "\n",
        "fig.update_layout(\n",
        "    width = 400,\n",
        "    height = 400,)\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "4991c9db",
      "metadata": {},
      "source": [
        "\n",
        "### 1. ¿El tiempo que se tarda el algoritmo en terminar, comparado a lo que vimos en clase?\n",
        "\n",
        "    Comparado a lo que vimos en clase: No hay un cambio significativo en el tiempo de ejecución. La operación adicional de dividir por 2 no es computacionalmente costosa, por lo que eliminarla no acelera el proceso de manera apreciable.\n",
        "\n",
        "### 2. Si la pérdida se minimiza igual que lo que vimos en clase\n",
        "\n",
        "    Sí, la pérdida todavía se minimiza. Aunque los valores absolutos de la función de pérdida son diferentes, la dirección del gradiente y, por lo tanto, la actualización de los pesos y sesgos, es la misma.\n",
        "\n",
        "### 3. Si los pesos y sesgos son parecidos a los vistos en clase\n",
        "\n",
        "    Sí, los pesos y sesgos son muy similares a los que se obtuvieron con la función de pérdida original. Esto se debe a que la dirección del gradiente no cambia cuando eliminas la división por 2.\n",
        "\n",
        "### 4. Si el problema se resuelve como ocurrió en clase\n",
        "\n",
        "    Sí, el problema aún se resuelve correctamente. La función de pérdida todavía está minimizando la diferencia entre las predicciones del modelo y los valores reales, por lo que aún estamos ajustando los pesos y sesgos para mejorar nuestras predicciones.\n",
        "\n",
        "### 5. Si se obtiene un mejor resultado al hacer más iteraciones\n",
        "\n",
        "    No necesariamente. Hacer más iteraciones no garantiza mejores resultados, ya que el modelo puede haberse estabilizado y no mejora más después de cierto punto. Además, demasiadas iteraciones pueden llevar a un sobreajuste, lo que significa que el modelo se desempeña muy bien en los datos de entrenamiento pero no se generaliza bien a nuevos datos."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "f6597f81",
      "metadata": {},
      "source": [
        "## 5. Cambie la función de pérdida de la “L2-norm” a la “L1-norm”. Explique lo que ocurre en términos de:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3db2e6b6",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# Cambiando las observaciones a 1000\n",
        "\n",
        "observaciones = 1000\n",
        "\n",
        "x1 = np.random.uniform(low=-10, high=10, size=(observaciones,1))\n",
        "x2 = np.random.uniform(-10, 10, (observaciones,1))\n",
        "\n",
        "X = np.column_stack((x1,x2))\n",
        "\n",
        "ruido = np.random.uniform(-1, 1, (observaciones,1))\n",
        "\n",
        "metas = 2 * x1 - 3 * x2 + 5 + ruido\n",
        "\n",
        "x1N = x1.reshape(observaciones,)\n",
        "x2N = x2.reshape(observaciones,)\n",
        "metasN = metas.reshape(observaciones,)\n",
        "\n",
        "fig = px.scatter_3d(x = x1N, y = x2N, z = metasN)\n",
        "\n",
        "fig.update_layout(\n",
        "    width = 500,\n",
        "    height = 500,)\n",
        "\n",
        "fig.show()\n",
        "\n",
        "rango_inicial = 0.1\n",
        "\n",
        "pesos = np.random.uniform(low = -rango_inicial, high = rango_inicial, size=(2, 1))\n",
        "\n",
        "sesgos = np.random.uniform(low = -rango_inicial, high = rango_inicial, size=1)\n",
        "\n",
        "pesos.shape\n",
        "\n",
        "eta = 0.01\n",
        "\n",
        "for i in range (100):\n",
        "    y = np.dot(X, pesos) + sesgos\n",
        "    deltas = y - metas\n",
        "    # Cambiar a la norma L1\n",
        "    perdida = np.sum(np.abs(deltas)) / observaciones\n",
        "    print(perdida)\n",
        "    deltas_escaladas = deltas / observaciones\n",
        "    pesos = pesos - eta * np.dot(X.T, deltas_escaladas)\n",
        "    sesgos = sesgos - eta * np.sum(deltas_escaladas)\n",
        "\n",
        "print(pesos, sesgos)\n",
        "\n",
        "yN = y.reshape(observaciones,)\n",
        "metasN = metas.reshape(observaciones,)\n",
        "fig = px.scatter(x = yN, y =  metasN)\n",
        "\n",
        "fig.update_layout(\n",
        "    width = 400,\n",
        "    height = 400,)\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "d9509b5d",
      "metadata": {},
      "source": [
        "### 1. El tiempo que se tarda el algoritmo en terminar, comparado a lo que vimos en clase\n",
        "\n",
        "    En comparación toma mucho mas tiempo que el original ya que el primero toma solo 0.2 seg mientras que este toma 2.2seg, esto ya que en general, la norma L1 puede requerir más tiempo para converger a una solución, especialmente en problemas con ruido. Esto se debe a que es menos sensible a los valores atípicos.\n",
        "\n",
        "### 2. Si la pérdida se minimiza igual que lo que vimos en clase\n",
        "\n",
        "   Si, la pérdida se ha minimizado bastante bien, ya que los pesos y los sesgos son bastante cercanos a los valores que obtuvimos con anterioridad.\n",
        "\n",
        "### 3. Si los pesos y sesgos son parecidos a los vistos en clase\n",
        "\n",
        "    Si, Los pesos y sesgos obtenidos son bastante cercanos a los valores vistos en clase. En particular, los pesos están muy cerca de 2 y -3, respectivamente.\n",
        "    \n",
        "### 4. Si el problema se resuelve como ocurrió en clase\n",
        "\n",
        "    Sí, basándonos en los pesos y sesgos obtenidos, parece que el problema se ha resuelto de manera similar a cómo se hizo en clase.\n",
        "\n",
        "### 5. Si se obtiene un mejor resultado al hacer más iteraciones\n",
        "\n",
        "    No, se mantiene bastante similar a los resultados que obtuvimos con anterioridad.\n",
        "\n",
        "### 6. ¿Tendrá una de estas más limitaciones que la otra? \n",
        "\n",
        "    Ambas normas tienen sus propias ventajas y desventajas, y la elección entre las dos depende del problema y los datos específicos. En este caso, la norma L1 ha funcionado bastante bien."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68fed6bf",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# Cambiando las observaciones a 1000\n",
        "\n",
        "observaciones = 1000\n",
        "\n",
        "x1 = np.random.uniform(low=-10, high=10, size=(observaciones,1))\n",
        "x2 = np.random.uniform(-10, 10, (observaciones,1))\n",
        "\n",
        "X = np.column_stack((x1,x2))\n",
        "\n",
        "ruido = np.random.uniform(-1, 1, (observaciones,1))\n",
        "\n",
        "# Aquí cambiamos la función a f(x1,x2) = 13 * x1 + 7 * x2 - 12.\n",
        "metas = 13 * x1 + 7 * x2 - 12 + ruido\n",
        "\n",
        "x1N = x1.reshape(observaciones,)\n",
        "x2N = x2.reshape(observaciones,)\n",
        "metasN = metas.reshape(observaciones,)\n",
        "\n",
        "fig = px.scatter_3d(x = x1N, y = x2N, z = metasN)\n",
        "\n",
        "fig.update_layout(\n",
        "    width = 500,\n",
        "    height = 500,)\n",
        "\n",
        "fig.show()\n",
        "\n",
        "rango_inicial = 0.1\n",
        "\n",
        "pesos = np.random.uniform(low = -rango_inicial, high = rango_inicial, size=(2, 1))\n",
        "\n",
        "sesgos = np.random.uniform(low = -rango_inicial, high = rango_inicial, size=1)\n",
        "\n",
        "pesos.shape\n",
        "\n",
        "eta = 0.01\n",
        "\n",
        "for i in range (100):\n",
        "    y = np.dot(X, pesos) + sesgos\n",
        "    deltas = y - metas\n",
        "    perdida = np.sum(deltas ** 2) / 2 / observaciones\n",
        "    print(perdida)\n",
        "    deltas_escaladas = deltas / observaciones\n",
        "    pesos = pesos - eta * np.dot(X.T, deltas_escaladas)\n",
        "    sesgos = sesgos - eta * np.sum(deltas_escaladas)\n",
        "\n",
        "print(pesos, sesgos)\n",
        "\n",
        "yN = y.reshape(observaciones,)\n",
        "metasN = metas.reshape(observaciones,)\n",
        "fig = px.scatter(x = yN, y =  metasN)\n",
        "\n",
        "fig.update_layout(\n",
        "    width = 400,\n",
        "    height = 400,)\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "fefff7c0",
      "metadata": {},
      "source": [
        "### 1.¿Funciona el algoritmo de la misma forma?\n",
        "    Sí, el algoritmo funciona de la misma manera. Aunque cambiamos la función objetivo, el algoritmo de aprendizaje (descenso de gradiente) sigue siendo el mismo. En este caso, el algoritmo ha convergido a los pesos y sesgos correctos, lo que indica que ha aprendido correctamente la relación entre las variables de entrada y la variable objetivo."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "noteable": {
      "last_delta_id": "d58c41cf-1898-4d48-9bed-19f3a9c54f1f",
      "last_transaction_id": "d58c41cf-1898-4d48-9bed-19f3a9c54f1f"
    },
    "noteable-chatgpt": {
      "create_notebook": {
        "openai_conversation_id": "f882101f-d81d-5f06-bafb-46a895aa0ee5",
        "openai_ephemeral_user_id": "03eda62c-e1af-5e81-acab-e5c81353927f"
      }
    },
    "nteract": {
      "version": "noteable@2.9.0"
    },
    "selected_hardware_size": "small"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
